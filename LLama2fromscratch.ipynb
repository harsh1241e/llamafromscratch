{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook i will implement Llama2 from scratch and load pre trained weights and also make inferences . I will run it on cpu ( bcoz of size issue ) . I implemented Grouped query attention and KV cache."
      ],
      "metadata": {
        "id": "4uYedqDAPofE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQjMUkrsFWfq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Optional"
      ],
      "metadata": {
        "id": "L7fOLgWbtKC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ModelArgs:\n",
        "  dim : int = 4096\n",
        "  n_layers : int = 32\n",
        "  n_heads : int = 32 #no. of heads for queries\n",
        "  n_kv_heads : Optional[int] = None  # no. of heads for keys\n",
        "  vocab_size : int = -1\n",
        "  multiple_of : int = 256\n",
        "  ffn_dim_multiplier : Optional[float] = None\n",
        "  norm_eps : float = 1e-5\n",
        "  max_batch_size : int = 32\n",
        "  max_seq_len : int  = 2048\n",
        "  device : str = None"
      ],
      "metadata": {
        "id": "gJ18ITyCtSOb",
        "outputId": "e427384d-7969-4b81-dd72-4acde8318bc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataclass' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-037d8d5cf3f0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mdataclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mModelArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mdim\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mn_layers\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mn_heads\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m \u001b[0;31m#no of heads for queries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataclass' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def precompute_theta_pos_frequencies(head_dim : int , seq_len : int , device : str , theta : float = 10000) :\n",
        "  #head_Dim = dim / H\n",
        "  assert head % 2 == 0\n",
        "  theta_numerator = torch.arange(0 , head_dim , 2 ).float()\n",
        "  m = torch.arange(0, seq_len , , device = device)\n",
        "  freqs = torch.outer(m , theta ).float()\n",
        "  freq_complex = torch.polar(torch.ones_like(freqs) ,  freqs )\n",
        "  return freq_complex"
      ],
      "metadata": {
        "id": "V_UnaQJdytNs",
        "outputId": "acc3065c-3e1a-4e83-e769-d0e3cde6a110",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-2-6dd62ba7c001>, line 5)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-6dd62ba7c001>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    m = torch.arange(0, seq_len , , device = device)\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_rotatry_embeddings(x: torch.Tensor, freqs_complex: torch.Tensor, device: str):\n",
        "   # (B, Seq_Len, H, Head_Dim) -> (B, Seq_Len, H, Head_Dim/2)\n",
        "  x_complex = torch.view_as_complex(x.float().reshape(*x.shpae[:-1] , -1 , 2 ) )\n",
        "  #(Seq_len , Head_Dim / 2 ) - > (1 , seq_len , 1 , Head_Dim/2)\n",
        "  freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
        "  x_rotated = x_complex * freqs_complex\n",
        "  x_out = torch.view_as_real(x_rotated)\n",
        "  x_out = x_out.reshape(*x.shape)\n",
        "  return x_out.type_as(x).to(device)"
      ],
      "metadata": {
        "id": "zvZAExqbMYX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(nn.Module):\n",
        "  def __init__(self , dim : int , eps : float = 1e-6 )\n",
        "      self.dim = dim\n",
        "      self.eps = eps\n",
        "      self.weights = nn.Parameter(torch.ones(dim))\n",
        "  def _norm(self , x):\n",
        "    return x * torch.rsqrt((x.pow(2)).mean(-1 , keepd_dim = True ) + self.eps)\n",
        "  def forward(self , x ):\n",
        "    return self.weights * self._norm(x)\n"
      ],
      "metadata": {
        "id": "nPiCUTF3Ns1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self , args : ModelArgs):\n",
        "    super().__init__()\n",
        "    hidden_dim = 4 * args.dim\n",
        "    hidden_size = int(2* hidden_dim / 3)\n",
        "    if args.ffn_dim_multiplier is not None :\n",
        "      hidden_dim = int(args.ffn_dim_multiplier * hidden_dim )\n",
        "    hidden = args.multiple_of * ((hidden_dim + args.multiple_of - 1) // args.multiple_of)\n",
        "    self.w1 = nn.Linear(args.dim , hidden_dim , bias = False )\n",
        "    self.w3 = nn.Linear(args.dim , hidden_dim , bias = False)\n",
        "    self.w2 = nn.Linear(hidden_dim , args.dim , bias = False)\n",
        "  def forward(self , x : torch.Tensor ):\n",
        "    swish = F.silu(self.w1(x))\n",
        "    x_v = self.w3(x)\n",
        "    return self.w2(swish * x_v )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MzWc0xD5vrFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def repeat_kv(x : torch.Tensor , n_rep = int ) -> torch.Tensor :\n",
        "  batch_size , seq_len , n_kv_heads , head_dim = x.shape\n",
        "  if n_rep == 1 :\n",
        "    return x\n",
        "  else :\n",
        "    #(B , Seq_Len , N_KV_Heads , 1 , Head_Dim)\n",
        "    return (\n",
        "        x[: ,: , : , None , : ].expand(batch_size ,seq_len , n_kv_heads ,\n",
        "         n_rep , head_dim ).reshape(batch_size ,seq_len , n_kv_heads * n_rep, head_dim)\n",
        "    )"
      ],
      "metadata": {
        "id": "iII0tD_hdtN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self , args : ModelArgs):\n",
        "    super().__init__()\n",
        "    self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
        "    self.n_heads_q = args.n_heads\n",
        "\n",
        "    self.head_dim = args.dim // args.n_heads\n",
        "    self.n_rep = self.n_heads_q // self.n_kv_heads\n",
        "\n",
        "    self.wq = nn.Linear(args.dim , self.n_heads * self.head_dim , bias = False )\n",
        "    self.wk = nn.Linear(args.dim , self.n_kv_heads * self.head_dim , bias = False)\n",
        "    self.wv = nn.Linear(args.dim , self.n_kv_heads * self.head_dim , bias = False )\n",
        "    self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
        "\n",
        "    self.cache_k = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))\n",
        "    self.cache_v = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))\n",
        "  def forward(self , x : torch.Tensor , start_pos : int ,freqs_complex : torch.Tensor):\n",
        "    batch_size , seq_len , _ = x.shape\n",
        "    xq = self.wq(x)\n",
        "    xk = self.wk(x)\n",
        "    xv = self.wv(x)\n",
        "    ##\n",
        "    ####\n",
        "    xq = xq.view(batch_size ,seq_len , self.n_heads , self.head_dim)\n",
        "    xv = xv.view(batch_size , seq_len , self.n_kv_heads , self.head_dim)\n",
        "    xk = xk.view(batch_size , seq_len , self.n_kv_heads , self.head_dim)\n",
        "\n",
        "    xq = apply_rotatry_embeddings(xq , freq_complex , device = x.device )\n",
        "    xk = apply_rotatry_embeddings(xk , freq_complex , device = x.device )\n",
        "\n",
        "    #replace the entry in the cache for this token\n",
        "    self.cache_k[:batch_size , start_pos : start_pos + seq_len ] = xk     #seq_len = 1\n",
        "    self.cache_v[:batch_size , start_pos : start_pos + seq_len  ]  = xv\n",
        "\n",
        "\n",
        "    #retrieve all the cahced keys and values so far\n",
        "    keys = self.cache_k[:batch_size , 0: start_pos + seq_len]\n",
        "    values = self.cache_v[ : batch_size , 0 : start_pos + seq_len ]\n",
        "\n",
        "    #(B , seq_len_kv , )\n",
        "    #i just change the shape by repeating as implemented in llama code(however it may not be most optimized)\n",
        "\n",
        "    #repeat the heads of K and V to reach the number of heads of queries\n",
        "    keys = repeat_kv(keys , self.n_rep)\n",
        "\n",
        "    values = repeat_kv(values  , self.n_rep)\n",
        "\n",
        "    ########\n",
        "    # (B , 1 , H_Q , Head_Dim) -- > (B , H_Q , 1 , Head_Dim)\n",
        "    xq = xq.transpose(1 , 2)\n",
        "    keys = keys.transpose(1 , 2)\n",
        "    values = values.transpose(1 , 2)\n",
        "\n",
        "    scores = torch.matmul(xq , keys.transpose(2,3))/ math.sqrt(self.head_dim)\n",
        "    scores = F.softmax(scores.float() , dim = -1).type_as(xq)\n",
        "\n",
        "    # B , H_Q , 1 , Seq_Len_KV\n",
        "    out = torch.matmul(scores , values )\n",
        "    #B , H_Q , 1 , Head_dim\n",
        "    return self.w_o(out.contiguous().view(batch_size , seq_len  ,  -1 )) # batch , 1 , dim\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TlvBfgpdFfpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self , , args: ModelArgs ):\n",
        "    super().__init__()\n",
        "    self.dim = args.dim\n",
        "    self.n_heads = args.n_heads\n",
        "    self.head_dim  = args.dim // args.n_heads\n",
        "\n",
        "\n",
        "\n",
        "    self.self_attention = SelfAttention(args)\n",
        "    self.feed_forward = FeedForward(args)\n",
        "    self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "    self.attention_norm = RMSNorm(args.dim , eps = args.norm_eps )\n",
        "\n",
        "  def forward(self , x : torch.Tensor, start_pos: int, freqs_complex: torch.Tensor ):\n",
        "    h = x + self.attention.forward(\n",
        "            self.attention_norm(x), start_pos, freqs_complex\n",
        "        )\n",
        "    out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "9wkAHKGtSycY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.module):\n",
        "  def __init__(self , args : ModelArgs )  -> None :\n",
        "    super().__init__()\n",
        "    assert args.vocab_size != -1 , \" set the vocab size \"\n",
        "    self.vocab_size = args.vocab_size\n",
        "    self.n_layers = args.n_layers\n",
        "    self.tok_embeddings = nn.Embedding(self.vocab_size , args.dim)\n",
        "    self.layers = nn.ModuleList()\n",
        "    for _ in range(args.n_layers)\n",
        "      self.layers.append(EncoderBlock(args))\n",
        "    self.norm = RMSNorm(dim = args.dim , eps = args.norm_eps)\n",
        "    self.output = nn.Linear(args.dim , self.vocab_size , bias = False)\n",
        "    self.freqs_complex = precompute_theta_pos_frequencies(self.args.dim // self.args.n_heads , self.args.max_seq_len * 2 , device = self.args.device)\n",
        "  def forward(self , tokens : torch.Tensor , start_pos  : int ):\n",
        "    batch_size, seq_len = tokens.shape\n",
        "    assert seq_len == 1, \"Only one token at a time can be processed\"\n",
        "\n",
        "    h = self.tok_embeddings(tokens)\n",
        "\n",
        "    freqs_complex = self.freqs_complex[start_pos:start_pos + seq_len]\n",
        "\n",
        "\n",
        "    for layer in self.layers:\n",
        "      h = layer(h, start_pos, freqs_complex)\n",
        "    h = self.norm(h)\n",
        "    output = self.output(h).float()\n",
        "    return output\n",
        "\n"
      ],
      "metadata": {
        "id": "frtV7gd7QaqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inferencing"
      ],
      "metadata": {
        "id": "G4V-5vvOzzTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Opitonal\n",
        "import torch\n",
        "import time\n",
        "from pathlib import Path\n",
        "import json\n",
        "from sentencepiece import SentencePieceProcessor\n",
        "from tqdm import tqdm\n",
        "import os"
      ],
      "metadata": {
        "id": "3_5XwZaGzyDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LLaMa:\n",
        "  def __init__(self , model : Transformer , tokenizer : SentencePieceProcessor , model_args : ModelArgs) -> None :\n",
        "    self.model = model\n",
        "    self.tokenizer = tokenizer\n",
        "    self.args = model_args\n",
        "  @staticmethod\n",
        "  def build(checkpint)"
      ],
      "metadata": {
        "id": "v-8sT4yi0FM8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}